## # Resource Ingestion Architecture

**Scalable ETL pipeline - add new sources in 50 lines of code**

---

## Problem: One Parser Per Source Doesn't Scale

### âŒ Bad Architecture (What NOT to Do)
```
convex/ingestion/
â”œâ”€â”€ nys_oaa_parser.ts          â† 370 lines (80% duplicate)
â”œâ”€â”€ eldercare_locator_parser.ts â† 400 lines (80% duplicate)
â”œâ”€â”€ benefitscheckup_parser.ts   â† 350 lines (80% duplicate)
â”œâ”€â”€ 211_la_parser.ts            â† 380 lines (80% duplicate)
â””â”€â”€ findhelp_parser.ts          â† 420 lines (80% duplicate)

Total: 1,920 lines (80% is copy-paste) ðŸ˜±
```

**Problems**:
1. ðŸ”¥ **Code duplication**: Phone normalization copied 5 times
2. ðŸ› **Bug multiplication**: Fix phone parsing â†’ Need to fix in 5 files
3. ðŸ“ˆ **Linear complexity**: 10 sources = 3,800 lines
4. ðŸ• **Maintenance nightmare**: Change category mapping â†’ Update all files

---

## Solution: 3-Layer Pipeline (DRY)

### âœ… Good Architecture (What TO Do)

```
convex/ingestion/
â”œâ”€â”€ shared/                     â† REUSED by all sources
â”‚   â”œâ”€â”€ types.ts               (60 lines)
â”‚   â”œâ”€â”€ normalize.ts           (300 lines) â† All normalization logic
â”‚   â””â”€â”€ load.ts                (150 lines) â† All database logic
â”‚
â”œâ”€â”€ adapters/                   â† SOURCE-SPECIFIC (50-100 lines each)
â”‚   â”œâ”€â”€ nysOaaAdapter.ts       (50 lines)
â”‚   â”œâ”€â”€ eldercareLocatorAdapter.ts (40 lines)
â”‚   â”œâ”€â”€ openReferralAdapter.ts (50 lines)
â”‚   â”œâ”€â”€ benefitsCheckUpAdapter.ts (60 lines)
â”‚   â””â”€â”€ findhelpAdapter.ts     (45 lines)
â”‚
â””â”€â”€ importResources.ts          (100 lines) â† Universal importer

Total: 755 lines (10 sources would be ~1,000 lines vs 3,800 lines!)
```

**Benefits**:
1. âœ… **No duplication**: Normalization logic written once
2. âœ… **Easy fixes**: Bug in phone parsing â†’ Fix in one place
3. âœ… **Constant complexity**: 10 sources = +500 lines (not +3,800)
4. âœ… **Easy maintenance**: Change category mapping â†’ Update `normalize.ts` only

---

## Layer 1: Source Adapters (Format-Specific)

**Purpose**: Parse source-specific format â†’ Common intermediate format

**Example**: NYS OAA Adapter
```typescript
// convex/ingestion/adapters/nysOaaAdapter.ts (50 lines)

export function parseNysOaa(fileContent: string): IntermediateRecord[] {
  const sections = fileContent.split(/\n\n+/);

  return sections.map(section => {
    const lines = section.split('\n');

    return {
      title: lines[0],
      description: lines.slice(1).join(' '),
      providerName: extractField(section, 'Provider:'),
      phone: extractField(section, 'Telephone:'),
      email: extractField(section, 'Email:'),
      // ... etc
    };
  });
}
```

**Key point**: Only 50 lines of source-specific parsing logic!

---

## Layer 2: Shared Normalization (Generic)

**Purpose**: Transform intermediate format â†’ Clean, categorized, normalized data

**Handles** (300 lines, used by ALL sources):
- Phone normalization: `(315) 946-5624` â†’ `+13159465624`
- URL normalization: `www.example.com` â†’ `https://www.example.com`
- Address parsing: Extract ZIP, city, state from messy strings
- **Categorization**: Keywords â†’ Resource categories
- **Pressure zone mapping**: Categories â†’ Zones
- Sector inference: Provider name â†’ public/nonprofit/faith-based
- Eligibility extraction: Text â†’ Structured eligibility

**Example**:
```typescript
// convex/ingestion/shared/normalize.ts

export function normalizeRecord(
  raw: IntermediateRecord,
  metadata: { dataSourceType, aggregatorSource, license }
): NormalizedRecord {
  // Phone normalization (works for ALL sources)
  const phoneE164 = normalizePhone(raw.phone);

  // Categorization (works for ALL sources)
  const text = `${raw.title} ${raw.description}`;
  const categories = categorizeService(text);
  const zones = mapCategoriesToZones(categories);

  // ... etc
}
```

---

## Layer 3: Shared Database Loader (Generic)

**Purpose**: Insert normalized data â†’ Convex database

**Handles** (150 lines, used by ALL sources):
- **Deduplication**: Check if provider exists (by name)
- **Graph construction**: provider â†’ program â†’ facility â†’ serviceArea â†’ resource
- **Error handling**: Failed records don't break entire import

**Example**:
```typescript
// convex/ingestion/shared/load.ts

export async function loadNormalizedRecord(
  ctx: any,
  record: NormalizedRecord
): Promise<{ providerId, facilityId, programId, resourceId }> {
  // 1. Deduplication
  const existingProvider = await ctx.db
    .query("providers")
    .withIndex("by_name", q => q.eq("name", record.provider.name))
    .first();

  const providerId = existingProvider?._id ||
    await ctx.db.insert("providers", record.provider);

  // 2. Create facility, program, serviceArea, resource
  // ... (same logic for ALL sources)
}
```

---

## Adding a New Source (Example: BenefitsCheckUp)

### Step 1: Create Adapter (50 lines)
```typescript
// convex/ingestion/adapters/benefitsCheckUpAdapter.ts

interface BenefitsCheckUpRecord {
  programName: string;
  organization: string;
  phoneNumber: string;
  // ... etc
}

export function parseBenefitsCheckUp(
  apiResponse: BenefitsCheckUpRecord[]
): IntermediateRecord[] {
  return apiResponse.map(item => ({
    title: item.programName,
    description: item.description,
    providerName: item.organization,
    phone: item.phoneNumber,
    // ... map fields to IntermediateRecord
  }));
}
```

### Step 2: Register in Universal Importer (5 lines)
```typescript
// convex/ingestion/importResources.ts

// Add to union type
args: {
  source: v.union(
    v.literal("nys_oaa"),
    v.literal("eldercare_locator"),
    v.literal("benefits_checkup") // â† Add here
  )
}

// Add case
case "benefits_checkup":
  intermediateRecords = parseBenefitsCheckUp(data);
  break;
```

### Step 3: Done! ðŸŽ‰

**Total new code**: 55 lines

**Reused code**: 450 lines (normalize + load)

**Compare to one-off parser**: Would be ~370 lines (87% of which is duplicate)

---

## Directory Structure

```
convex/ingestion/
â”‚
â”œâ”€â”€ shared/                     â† 510 lines (SHARED)
â”‚   â”œâ”€â”€ types.ts               (60 lines)
â”‚   â”‚   â””â”€â”€ IntermediateRecord
â”‚   â”‚   â””â”€â”€ NormalizedRecord
â”‚   â”‚
â”‚   â”œâ”€â”€ normalize.ts           (300 lines)
â”‚   â”‚   â”œâ”€â”€ normalizePhone()
â”‚   â”‚   â”œâ”€â”€ normalizeUrl()
â”‚   â”‚   â”œâ”€â”€ parseAddress()
â”‚   â”‚   â”œâ”€â”€ categorizeService() â† Keywords â†’ Categories
â”‚   â”‚   â”œâ”€â”€ mapCategoriesToZones() â† Categories â†’ Zones
â”‚   â”‚   â”œâ”€â”€ inferSector()
â”‚   â”‚   â”œâ”€â”€ extractEligibility()
â”‚   â”‚   â””â”€â”€ normalizeRecord() â† Main function
â”‚   â”‚
â”‚   â””â”€â”€ load.ts                (150 lines)
â”‚       â”œâ”€â”€ loadNormalizedRecord()
â”‚       â””â”€â”€ loadNormalizedRecords()
â”‚
â”œâ”€â”€ adapters/                   â† 50-100 lines EACH
â”‚   â”œâ”€â”€ nysOaaAdapter.ts       (50 lines)
â”‚   â”‚   â””â”€â”€ parseNysOaa()
â”‚   â”‚
â”‚   â”œâ”€â”€ eldercareLocatorAdapter.ts (40 lines)
â”‚   â”‚   â””â”€â”€ parseEldercareLocator()
â”‚   â”‚
â”‚   â”œâ”€â”€ openReferralAdapter.ts (50 lines)
â”‚   â”‚   â””â”€â”€ parseOpenReferral()
â”‚   â”‚
â”‚   â”œâ”€â”€ benefitsCheckUpAdapter.ts (60 lines)
â”‚   â”‚   â””â”€â”€ parseBenefitsCheckUp()
â”‚   â”‚
â”‚   â””â”€â”€ findhelpAdapter.ts     (45 lines)
â”‚       â””â”€â”€ parseFindhelp()
â”‚
â””â”€â”€ importResources.ts          (100 lines)
    â”œâ”€â”€ importResources() â† Universal importer
    â”œâ”€â”€ importNysOaa() â† Convenience wrapper
    â”œâ”€â”€ importEldercareLocator()
    â””â”€â”€ importOpenReferral()
```

---

## Usage Examples

### Import NYS OAA Data
```typescript
const fileContent = fs.readFileSync('source/food.md', 'utf-8');

await ctx.runMutation(api.ingestion.importResources.importNysOaa, {
  fileContent
});

// Result: 85 resources imported
```

### Import Eldercare Locator API
```typescript
const response = await fetch('https://eldercare.acl.gov/api/search?zip=94102');
const data = await response.json();

await ctx.runMutation(api.ingestion.importResources.importEldercareLocator, {
  apiResponse: data
});

// Result: 50 resources imported
```

### Import Open Referral (211 Data)
```typescript
const services = await fetch('https://api.211.org/services').then(r => r.json());

await ctx.runMutation(api.ingestion.importResources.importOpenReferral, {
  services
});

// Result: 200 resources imported
```

---

## Key Takeaways

1. âœ… **Source adapters are tiny** (50-100 lines each)
   - Only parse format-specific data
   - Output common `IntermediateRecord` format
   - No duplication

2. âœ… **Normalization is shared** (300 lines, reused)
   - Phone/URL/address normalization
   - Categorization logic (keywords â†’ categories)
   - Pressure zone mapping (categories â†’ zones)
   - Works for ALL sources

3. âœ… **Database loading is shared** (150 lines, reused)
   - Deduplication (check existing providers)
   - Graph construction (5 tables)
   - Error handling
   - Works for ALL sources

4. âœ… **Adding sources is trivial**
   - New source = 50 lines of adapter + 5 lines to register
   - No changes to normalization or loading
   - Scale to 100+ sources without code explosion

---

## Scaling Roadmap

### Phase 1: Manual MVP (Week 1-2)
- âœ… NYS OAA adapter (done)
- â³ Eldercare Locator adapter
- â³ 211 LA County adapter
- **Goal**: 200 resources covering 5 pressure zones

### Phase 2: API Automation (Month 1)
- â³ BenefitsCheckUp adapter
- â³ findhelp.org adapter
- â³ VA Caregiver Support adapter
- **Goal**: 2,000 resources, daily automated syncs

### Phase 3: User Submissions (Month 2+)
- â³ User submission form â†’ IntermediateRecord
- â³ Admin verification workflow
- â³ Quarterly re-verification (stale data detection)
- **Goal**: Living directory, quality > quantity

---

## Comparison: One-Off vs Modular

| Metric | One-Off Parsers | Modular Pipeline |
|--------|----------------|------------------|
| **Initial cost** (3 sources) | 1,100 lines | 755 lines |
| **Add 4th source** | +370 lines | +50 lines |
| **Add 10th source** | +370 lines | +50 lines |
| **Total (10 sources)** | 3,700 lines | 1,255 lines |
| **Fix phone bug** | Change 10 files | Change 1 file |
| **Update category mapping** | Change 10 files | Change 1 file |
| **Onboard new dev** | Learn 10 parsers | Learn 1 pattern |

**Verdict**: Modular pipeline is **3x smaller** at scale and **10x easier** to maintain.

---

**Last updated**: 2025-10-14 by Claude Code
