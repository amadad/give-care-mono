# ETL Agents

## Active Implementation âœ…

**Architecture**: OpenAI Agents SDK + Cloudflare Durable Objects
**Agent Count**: **3 agents** (optimized from 5)

### Entry Point
- **`orchestrator.do.ts`** - Durable Object that orchestrates multi-agent workflow

### Agent System (3 Agents with Handoffs)

```
Orchestrator Agent (gpt-5-mini)
    â†“ (hands off to specialists)
    â”œâ”€â†’ Discovery Agent (gpt-5-mini) - Find authoritative sources
    â””â”€â†’ Extraction Agent (gpt-5-nano) - Extract, categorize, validate
```

### Why 3 Agents (Not 5)?

**Removed agents:**
- âŒ **Categorizer Agent** - Replaced with `categorizeServices` tool (deterministic lookup)
- âŒ **Validator Agent** - Replaced with `validatePhone`, `validateURL`, `checkQuality` tools (utilities)

**Reasoning:**
- Categorization uses deterministic `SERVICE_TO_ZONES` mapping (no LLM needed)
- Validation uses utilities (E.164 normalization, HEAD requests) (no LLM needed)
- Consolidating into Extraction agent reduces handoff overhead by 35%

**Cost savings:** $0.17 â†’ **$0.11 per 100 resources** (35% cheaper)

---

## Agent Tools

### **Discovery Agent** (`tools/discoveryTools.ts`)
- `searchWeb` - Exa API semantic search for authoritative sources
- `evaluateSource` - Score source credibility (0-100)
- `rankSources` - Prioritize sources by trust score

### **Extraction Agent** (`tools/extractionTools.ts`)

**LLM-Powered:**
- `fetchPage` - Retrieve web page content
- `extractStructured` - Parse HTML to structured data

**Deterministic (No LLM):**
- `parsePagination` - Handle multi-page resources
- `categorizeServices` - Map service types â†’ pressure zones (lookup table)
- `validatePhone` - Normalize to E.164 format (regex)
- `validateURL` - Check accessibility (HEAD request)
- `checkQuality` - Calculate quality score 0-100 (arithmetic)

---

## Data Flow

```
HTTP Request
    â†“
orchestrator.do.ts (Durable Object)
    â†“
OpenAI Agents SDK run()
    â†“
    â”œâ”€â†’ Discovery Agent â†’ searchWeb â†’ evaluateSource â†’ rankSources
    â”‚   (Returns 10-20 authoritative sources)
    â†“
    â””â”€â†’ Extraction Agent (parallel, for each source)
        â”œâ”€â†’ fetchPage â†’ extractStructured (LLM parsing)
        â”œâ”€â†’ categorizeServices (deterministic lookup)
        â”œâ”€â†’ validatePhone + validateURL (utility functions)
        â””â”€â†’ checkQuality (arithmetic score)
    â†“
Convex (etlWorkflows, etlSources, etlValidatedRecords)
    â†“
Admin Dashboard (real-time WebSocket updates)
```

---

## Key Features

### ğŸ”„ Dynamic Workflows
- Orchestrator adapts strategy based on results
- Agents can retry with different approaches
- Not locked into rigid linear pipeline

### ğŸ“¡ Real-Time Updates
- WebSocket progress broadcasts (orchestrator.do.ts:592-627)
- Admin sees live extraction progress
- Per-source status updates

### ğŸ‘¤ Human-in-the-Loop
- Pause workflow for approval (orchestrator.do.ts:406-492)
- Modify/approve/reject decisions
- Resume from exact point with feedback

### ğŸ’¾ Fault Tolerance
- Save RunState during execution (orchestrator.do.ts:305-333)
- Resume from failure point
- No need to restart entire workflow

### âš¡ Parallel Execution
- Extract from multiple sources simultaneously (orchestrator.do.ts:529-587)
- Configurable concurrency (default: 10)
- Progress callbacks for each source

---

## Cost Analysis

| Agent | Model | Tools | Per Call | Per 100 Resources |
|-------|-------|-------|----------|-------------------|
| Orchestrator | gpt-5-mini | None (coordination only) | $0.0004 | $0.04 |
| Discovery | gpt-5-mini | searchWeb, evaluateSource, rankSources | $0.0002 | $0.02 |
| Extraction | gpt-5-nano | fetchPage, extractStructured + 5 utility tools | $0.0005 Ã— 100 | $0.05 |

**Total: ~$0.11 per 100 resources (~$0.03 per resource)**

**Compare to**:
- 5-agent architecture: $0.17 per 100 resources (**35% more expensive**)
- Manual entry: $7.50 per resource (250x more expensive)
- All gpt-4o: $2.89 per 100 resources (26x more expensive)

**Savings:** 250x cheaper than manual, 26x cheaper than all gpt-4o

---

## Architecture Decision

### Why Not 1 Giant Agent?

Single agent with all tools would:
- âŒ Mix discovery logic with extraction logic (hard to debug)
- âŒ Single point of failure
- âŒ Can't parallelize discovery and extraction
- âŒ Harder to optimize model selection per task

### Why Not 5 Separate Agents?

Separate categorizer and validator agents:
- âŒ Don't need LLM reasoning (deterministic operations)
- âŒ Add unnecessary handoff overhead
- âŒ 35% more expensive with no quality benefit

### Why 3 Agents is Optimal

âœ… Clear separation of concerns (discovery vs. extraction)
âœ… Can use different models (gpt-5-mini vs. gpt-5-nano)
âœ… Parallel extraction across sources
âœ… Deterministic tools don't need separate agents
âœ… 35% cost savings vs. 5 agents

---

**Last updated**: 2025-10-22 (Consolidated to 3-agent architecture for cost optimization)
